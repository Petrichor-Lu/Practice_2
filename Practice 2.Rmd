```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(tidytext)
library(textstem)
library(clinspacy)
library(topicmodels)
library('reshape2')
library(stringr)

```

This practical is based on exploratory data analysis, named entity recognition, and topic modelling of unstructured medical note free-text data derived from electronic medical records (EMR).  Real EMR data is very difficult to access without a specific need/request so this data set is derived from [medical transcription](https://mtsamples.com/) data instead.  I'll also caveat that the options of natural language processing (NLP) in R are far inferior to those available in Python. 

First, install the packages in the setup block (`install.packages(c("readr", "dplyr", "tidyr", "ggplot2", "tidtext", "textstem", "clinspacy", "topicmodels", "reshape2"))`).

Note: To try and make it clearer which library certain functions are coming from clearer, I'll try to do explicit imports throughout this notebook.

## Data Parsing

After that we can grab the dataset directly from the `clinspacy` library.

```{r}
raw.data <- clinspacy::dataset_mtsamples()
dplyr::glimpse(raw.data)
```

There is no explanation or data dictionary with this dataset, which is a surprisingly common and frustrating turn of events!  

**1** Using the output of dplyr's `glimpse` command (or rstudio's data viewer by clicking on `raw.data` in the Environment pane) provide a description of what you think each variable in this dataset contains.

**1. note_id: <int>, Integer type, representing a unique identifier for each record. Each record has a unique ID, starting from 1 and incrementing.     
description: <chr>, Character type, providing a brief summary of the patient's chief complaint or diagnosis. This typically explains the reason for the patient's visit.    
medical_specialty: <chr>, Character type, indicating the medical specialty related to the medical record, such as "Allergy / Immunology," "Bariatrics," etc. This helps categorize records by different medical fields.     
sample_name: <chr>, Character type, representing the specific name of each medical record, such as "Allergic Rhinitis," "Laparoscopic Gastric Bypass," etc. This usually names the specific medical case or diagnosis.     
transcription: <chr>, Character type, containing the detailed medical record or transcription text, describing the patient's history, symptoms, examination results, etc. This is the main text data containing rich clinical information.     
keywords: <chr>, Character type, containing keywords related to the medical record, helping quickly understand the main content of the record. These keywords can be used for quick search and filtering of records.**


Let's see how many different medical specialties are featured in these notes: 
```{r}
raw.data %>% dplyr::select(medical_specialty) %>% dplyr::n_distinct()
```

So, how many transcripts are there from each specialty:


```{r}
ggplot2::ggplot(raw.data, ggplot2::aes(y=medical_specialty)) + ggplot2::geom_bar() + labs(x="Document Count", y="Medical Speciality")
```

Let's make our life easier and filter down to 3 specialties: a diagonstic/lab, a medical, and a surgical specialty

```{r} 
filtered.data <- raw.data %>% dplyr::filter(medical_specialty %in% c("Orthopedic", "Radiology", "Surgery")) 
```

## Text Processing

Let's now apply our standard pre-processing to the transcripts from these specialties.  
We are going to use the `tidytext` package to tokenise the transcript free-text.  
Let's remove stop words first. e.g., "the", "of", "to", and so forth. These are known as stop words and we can remove them relative easily using a list from  `tidytext::stop_words` and `dplyr::anti_join()`

```{r}

analysis.data <- filtered.data %>%
  unnest_tokens(word, transcription) %>%
  mutate(word = str_replace_all(word, "[^[:alnum:]]", "")) %>%
  filter(!str_detect(word, "[0-9]")) %>%
  anti_join(stop_words) %>%
  group_by(note_id) %>%
  summarise(transcription = paste(word, collapse = " ")) %>%
  left_join(select(filtered.data, -transcription), by = "note_id")
```


Now let's tokenize the `transcription` to words (unigram) 
By default this tokenises to words but other options include characters, n-grams, sentences, lines, paragraphs, or separation around a regular expression.

```{r}
tokenized.data.unigram <- analysis.data %>% tidytext::unnest_tokens(word, transcription, to_lower=TRUE)
```

You can also do bi-grams
```{r}
tokenized.data <- analysis.data %>% tidytext::unnest_tokens(ngram, transcription, token = "ngrams", n=2, to_lower = TRUE)
```

How many stop words are there in `tidytext::stop_words` from each lexicon?
```{r}
tidytext::stop_words %>% dplyr::group_by(lexicon) %>% dplyr::distinct(word) %>% dplyr::summarise(n=dplyr::n())
```

**2** How many unique unigrams are there in the transcripts from each specialty:
**Coding as shown below**
```{r}
# Count the number of unique unigrams for each specialty
unique_unigrams_per_specialty <- tokenized.data.unigram %>% 
  dplyr::group_by(medical_specialty) %>% 
  dplyr::distinct(word) %>% 
  dplyr::summarise(unique_unigrams = dplyr::n())

# Show the result
unique_unigrams_per_specialty
```

Let's plot some distribution of unigram tokens (words)


```{r}
word_counts <- tokenized.data.unigram %>%
    group_by(word) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Words",
       y = "Number of Words")
```


Let's plot some distribution of bigram tokens (words)


```{r}
word_counts <- tokenized.data %>%
    group_by(ngram) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Bigrams",
       y = "Number of Words")
```

**3** How many unique bi-grams are there in each category without stop words and numbers?
**The resules as shown below**
```{r}
# Tokenize the transcription to bi-grams directly from the cleaned data
tokenized.data.bigrams <- analysis.data %>% 
  tidytext::unnest_tokens(ngram, transcription, token = "ngrams", n = 2, to_lower = TRUE)

# Count unique bi-grams for each category
unique_bigrams_per_category <- tokenized.data.bigrams %>%
  group_by(medical_specialty) %>%
  distinct(ngram) %>%
  summarise(unique_bigrams = n())

# Show the result
unique_bigrams_per_category
```

Sometimes we are interested in tokenising/segmenting things other than words like whole sentences or paragraphs.  
**4** How many unique sentences are there in each category? Hint: use `?tidytext::unnest_tokens` to see the documentation for this function.
**The results as shown below**
```{r}
# Tokenize the transcription to sentences from the cleaned data
tokenized.data.sentences <- analysis.data %>% 
  tidytext::unnest_tokens(sentence, transcription, token = "sentences", to_lower = TRUE)

# Count unique sentences for each category
unique_sentences_per_category <- tokenized.data.sentences %>%
  group_by(medical_specialty) %>%
  distinct(sentence) %>%
  summarise(unique_sentences = n())

# Show the result
unique_sentences_per_category
```


Now that we've tokenized to words and removed stop words, we can find the most commonly word used within each category:

```{r}
tokenized.data %>%
  dplyr::group_by(medical_specialty) %>%
  dplyr::count(ngram, sort = TRUE) %>%
  dplyr::top_n(5)
```

We should lemmatize the tokenized words to prevent over counting of similar words before further analyses.  
Annoyingly, `tidytext` doesn't have a built-in lemmatizer.

**5** Do you think a general purpose lemmatizer will work well for medical data? Why might it not?

**NO. I don't think so.    
1. Complexity of Medical Terminology: Some medical specialized terms that may not be in a general lemmatizer's dictionary.   
2. Abbreviations: Medical texts may use some abbreviations that may not be in a general lemmatizer's dictionary.    
3. Synonyms: Some synonyms may not be recognized and understood as synonyms by a general lemmatizer. **

Unfortunately, a specialised lemmatizer like in `clinspacy` is going to be very painful to install so we will just use a simple lemmatizer for now:

```{r}
lemmatized.data <- tokenized.data %>% dplyr::mutate(lemma=textstem::lemmatize_words(ngram))
```

We can now calculate the frequency of lemmas within each specialty and note.
```{r}
lemma.freq <- lemmatized.data %>% 
  dplyr::count(medical_specialty, lemma) %>%
  dplyr::group_by(medical_specialty) %>% 
  dplyr::mutate(proportion = n / sum(n)) %>%
  tidyr::pivot_wider(names_from = medical_specialty, values_from = proportion) %>%
  tidyr::pivot_longer(`Surgery`:`Radiology`,
               names_to = "medical_specialty", values_to = "proportion")
```

And plot the relative proportions 
```{r}

ggplot2::ggplot(lemma.freq, ggplot2::aes(x=proportion, 
                                         y=`Orthopedic`,
                                         color=abs(`Orthopedic` - proportion))) + 
  ggplot2::geom_abline(color="gray40", lty=2) +
  ggplot2::geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3) +
  ggplot2::geom_text(ggplot2::aes(label=lemma), check_overlap=TRUE, vjust=1.5) +
  ggplot2::scale_x_log10(labels=scales::percent_format()) + 
  ggplot2::scale_y_log10(labels=scales::percent_format()) + 
  ggplot2::scale_color_gradient(limits=c(0, 0.001), low="darkslategray4", high="gray75") +
  ggplot2::facet_wrap(~medical_specialty, ncol = 2) +
  ggplot2::theme(legend.position="none") +
  ggplot2:: labs(y="Orthopedic", x = NULL)
```
**6** What does this plot tell you about the relative similarity of lemma frequencies between Surgery and Orthopedic and between radiology and Orthopedic? Based on what these specialties involve, is this what you would expect?

**About the relative similarity of lemma frequencies:    
Surgery and Orthopedic: The lemma frequnencies between these two are similar, since these specialities use similar terminology, due to both invovling surgical procedures.    
Radiology and Orthopedic: The lemma frequencies between these two are less similar, since they focus on different specialties: Radiology on imaging and Orthopedic on bone treatment.      
Expectation:    
Yes, it's the same with what i expect. The resaon as i mentioned above.  **

**7** Modify the above plotting code to do a direct comparison of Surgery and Radiology (i.e., have Surgery or Radiology on the Y-axis and the other 2 specialties as the X facets)
**Coding as shown below**
```{r}
lemmatized.data <- tokenized.data %>% dplyr::mutate(lemma=textstem::lemmatize_words(ngram))
```

```{r}
lemma.freq <- lemmatized.data %>% 
  dplyr::count(medical_specialty, lemma) %>%
  dplyr::group_by(medical_specialty) %>% 
  dplyr::mutate(proportion = n / sum(n)) %>%
  tidyr::pivot_wider(names_from = medical_specialty, values_from = proportion) %>%
  tidyr::pivot_longer(`Orthopedic`:`Radiology`,
               names_to = "medical_specialty", values_to = "proportion")
```

```{r}
ggplot2::ggplot(lemma.freq, ggplot2::aes(x=proportion, 
                                         y=`Surgery`,
                                         color=abs(`Surgery` - proportion))) + 
  ggplot2::geom_abline(color="gray40", lty=2) +
  ggplot2::geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3) +
  ggplot2::geom_text(ggplot2::aes(label=lemma), check_overlap=TRUE, vjust=1.5) +
  ggplot2::scale_x_log10(labels=scales::percent_format()) + 
  ggplot2::scale_y_log10(labels=scales::percent_format()) + 
  ggplot2::scale_color_gradient(limits=c(0, 0.001), low="darkslategray4", high="gray75") +
  ggplot2::facet_wrap(~medical_specialty, ncol = 2) +
  ggplot2::theme(legend.position="none") +
  ggplot2:: labs(y="Surgery", x = NULL)

```

### TF-IDF Normalisation

Maybe looking at lemmas across all notes in a specialty is misleading, what if we look at lemma frequencies across a specialty.

```{r}
lemma.counts <- lemmatized.data %>% dplyr::count(medical_specialty, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(medical_specialty) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)
```
Now we can calculate the term frequency / invariant document frequency (tf-idf):

```{r}
all.counts.tfidf <- tidytext::bind_tf_idf(all.counts, lemma, medical_specialty, n) 
```

We can then look at the top 10 lemma by tf-idf within each specialty:

```{r}
all.counts.tfidf %>% dplyr::group_by(medical_specialty) %>% dplyr::slice_max(order_by=tf_idf, n=10)
```
**8** Are there any lemmas that stand out in these lists? Why or why not?
**Orthopedic：range motion (0.0014216458), carpal ligament(0.0008693518), transverse carpal(0.0008284411).     
    "range motion" means the movement capacity of joints.     "carpal ligament" means ligaments in the wrist.    "transverse carpal" means a disease related to orthopedic.    So, all of this three is highly related to the Orthopedic, showing that these words are frequently used in the field of Orthepedic.
Radiology: myocardial perfusion(0.0008513375), motor units(0.0006707507), calcific plaque(0.0005933564).
    "myocardial perfusion" means the blood flow to the heart muscle. "motor units" means the fundamental units of motor function in the nervous system. "calcific plaque" means the calcified deposits within arteries. All of this three is highly used and reloated to the Radiology, showing the top 3 requence of words used.
Surgery: anterior chamber(0.0004271596), lithotomy position(0.0003425736), external oblique(0.0002875926).
    "anterior chamber" means the fluid-filled space inside the eye between the cornea and the iris. "lithotomy position" means surgical position commonly used for procedures involving the pelvis and lower abdomen. "External oblique" means one of the muscles of the abdominal wall. All these three are highly related to the Surgery, showing that these words are frequently used in the field of Surgery.
    Since common lemmas appear frequently across various surgical procedures, their frequency is high, and thus they do not stand out. The "tf-idf" value combines term frequency with the rarity of the term across different documents. For these common lemmas, their high frequency is offset by their low rarity, as they occur in many documents, resulting in a lower "tf-idf" value for these terms.**


We can look at transcriptions in full using these lemmas to check how they are used with `stringr::str_detect`
```{r}
analysis.data %>% dplyr::select(medical_specialty, transcription) %>% dplyr::filter(stringr::str_detect(transcription, 'steri strips')) %>% dplyr::slice(1)
```

**9** Extract an example of one of the other "top lemmas" by modifying the above code
**Coding shown as below**
```{r}
analysis.data %>% dplyr::select(medical_specialty, transcription) %>%
  dplyr::filter(!stringr::str_detect(transcription, 'needle emgr')) %>% dplyr::slice(1)
```

## Topic Modelling

In NLP, we often have collections of documents (in our case EMR transcriptions) that we’d like to divide into groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.


- Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”


- Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.

First lets calculate a term frequency matrix for each transcription:
```{r}

lemma.counts <- lemmatized.data %>% dplyr::count(note_id, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(note_id) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)

emr.dcm <- all.counts %>% tidytext::cast_dtm(note_id, lemma, n)
```

Then we can use LDA function to fit a 5 topic (`k=5`) LDA-model
```{r}
emr.lda <- topicmodels::LDA(emr.dcm, k=5, control=list(seed=42))
emr.topics <- tidytext::tidy(emr.lda, matrix='beta')
```

Then we can extract the top terms per assigned topic:
```{r}

top.terms <- emr.topics %>% dplyr::group_by(topic) %>% 
  dplyr::slice_max(beta, n=10) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta)


top.terms %>% 
  dplyr::mutate(term=tidytext::reorder_within(term, beta, topic)) %>% 
  ggplot2::ggplot(ggplot2::aes(beta, term, fill=factor(topic))) + 
    ggplot2::geom_col(show.legend=FALSE) + 
    ggplot2::facet_wrap(~ topic, scales='free')  +
    ggplot2::theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1)) +
    tidytext::scale_y_reordered()
```



Now we can ask how well do these assigned topics match up to the medical specialties from which each of these transcripts was derived.

```{r}
specialty_gamma <- tidytext::tidy(emr.lda, matrix='gamma')

# we need to join in the specialty from the note_id
note_id_specialty_mapping <- lemmatized.data %>%
  dplyr::mutate(document=as.character(note_id)) %>% 
  dplyr::select(document, medical_specialty) %>% 
  dplyr::distinct()

specialty_gamma <- dplyr::left_join(specialty_gamma, note_id_specialty_mapping)
```

```{r}

specialty_gamma %>%
  dplyr::mutate(medical_specialty = reorder(medical_specialty, gamma * topic)) %>%
  ggplot2::ggplot(ggplot2::aes(factor(topic), gamma)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ medical_specialty) +
  ggplot2::labs(x = "topic", y = expression(gamma))
```

Interestingly, Surgery, Orthopedic, and Radiology assign mostly to a single topics. We'd possibly expect this from radiology due to referring to imaging for many different diagnoses/reasons. 
However, this may all just reflect we are using too few topics in our LDA to capture the range of possible assignments. 

**10** Repeat this with a 6 topic LDA, do the top terms from the 3 topic LDA still turn up? How do the specialties get split into sub-topics?

**Coding shown as below**
```{r}
# Create the matrix
dtm_matrix <- analysis.data %>%
  unnest_tokens(word, transcription) %>%
  count(note_id, word, sort = TRUE) %>%
  cast_dtm(note_id, word, n)

# 6-topic LDA
lda_6topics <- LDA(dtm_matrix, k = 6, control = list(seed = 1234))

top_terms_lda6 <- tidy(lda_6topics, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualize
top_terms_lda6 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) + 
    geom_col(show.legend = FALSE) + 
    facet_wrap(~ topic, scales = 'free')  +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    scale_y_reordered()
```

```{r}
# Extract the document-topic proportions
gamma_6topics <- tidy(lda_6topics, matrix = 'gamma')

# mapping of document IDs to medical specialties
doc_specialty_map <- analysis.data %>%
  mutate(document = as.character(note_id)) %>% 
  select(document, medical_specialty) %>% 
  distinct()

# Join the document-topic proportions with medical specialties
gamma_6topics <- left_join(gamma_6topics, doc_specialty_map, by = "document")

# Visualize
gamma_6topics %>%
  mutate(medical_specialty = reorder(medical_specialty, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ medical_specialty) +
  labs(x = "topic", y = expression(gamma))
```

## Credits

Examples draw heavily on material (and directly quotes/copies text) from Julia Slige's `tidytext` textbook.

